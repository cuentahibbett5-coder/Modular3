# Entrenamiento con M√°scara Ponderada y Filtrado de PDD Baja

## üéØ Objetivo

Mejorar el modelo de denoising enfatizando la regi√≥n cl√≠nicamente relevante (core) mientras se reduce el overfitting en zonas de ruido de Monte Carlo (periferia).

## üìä Estrategia Implementada

### 1. **Filtrado de PDD Baja (1%)**
- **Qu√© es PDD**: Percent Depth Dose = m√°xima dosis por capa de profundidad
- **Acci√≥n**: Eliminar el 1% inferior de capas (dosis m√°s baja)
- **Efecto**: Descarta capas donde el ruido monte carlo domina completamente
- **Beneficio**: Modelo no intenta aprender patrones ruidosos sin estructura real

### 2. **P√©rdida Ponderada por Dosis**
```
Loss = mean((pred - target)¬≤ √ó weights)

donde:
  weights = 1.0  si dosis ‚â• 20% del m√°ximo (CORE)
  weights = 0.5  si dosis <  20% del m√°ximo (PERIFERIA)
```

- **Core** (dosis alta): Peso total ‚Üí Aprende bien la estructura principal
- **Periferia** (dosis baja): Peso reducido ‚Üí No penaliza tanto los errores
- **Equilibrio**: Modelo aprende core perfectamente, ignora gracefully periferia ruidosa

---

## üîß Implementaci√≥n

### Script Principal: `train_weighted.py`

```python
# Dataset con filtrado PDD
train_ds = SimpleDoseDatasetWeighted(
    TRAIN_DIR, 
    DATASET_ROOT, 
    INPUT_LEVELS,
    PATCH_SIZE,
    percentile_pdd=1.0,  # Eliminar 1% inferior
    is_train=True
)

# Loss ponderado
def weighted_mse_loss(pred, tgt, weights):
    diff = (pred - tgt) ** 2
    weighted_diff = diff * weights
    return weighted_diff.mean()
```

### Caracter√≠sticas Clave:

‚úÖ **C√°lculo autom√°tico de PDD**
```python
pdd = np.array([np.max(vol[z]) for z in range(D)])
```

‚úÖ **M√°scara de dosis**
```python
weights = np.where(dose ‚â• threshold, 1.0, 0.5)
```

‚úÖ **Compatible con normalizaci√≥n por m√°ximo de input**
```python
# Normalizar AMBOS (input y target) por max(input)
# Permite modelo aprender amplificaci√≥n de dosis
```

---

## üìã Par√°metros de Configuraci√≥n

```python
DOSE_THRESHOLD = 0.20      # 20% del m√°ximo = l√≠mite core/periferia
LOW_WEIGHT     = 0.5       # Peso para periferia (< 20%)
HIGH_WEIGHT    = 1.0       # Peso para core (‚â• 20%)
PERCENTILE_PDD_LOW = 1.0   # Eliminar 1% inferior
```

Estos par√°metros pueden ajustarse en `train_weighted.py` seg√∫n sea necesario.

---

## üöÄ Ejecuci√≥n

### Local (si hay GPU):
```bash
python train_weighted.py
```

### En Cluster (SLURM):
```bash
sbatch run_train_weighted.sh
```

El script crear√° checkpoints en `runs/denoising_weighted/`

---

## üìä Evaluaci√≥n y Comparaci√≥n

### Comparar modelos:
```bash
python compare_models.py \
    --simple runs/denoising_v2/best.pt \
    --weighted runs/denoising_weighted/best.pt \
    --output comparison.png
```

Genera figura con:
- √âpoca en que se alcanz√≥ mejor loss
- Valor de val_loss para cada modelo
- An√°lisis cuantitativo de mejora

### Evaluar con normalizaci√≥n correcta:
```bash
python eval_correct_norm.py
```

Muestra:
- MAE, RMSE, correlaci√≥n por sample
- Ratio Pred/Target (¬øaprende a amplificar?)
- Tabla comparativa de predicciones

---

## üéì Interpretaci√≥n de Resultados

### Si val_loss(weighted) < val_loss(simple):
‚úÖ **El modelo weighted es mejor**
- Aprende mejor la estructura principal (core)
- Menos distorsionado por ruido perif√©rico
- Mejor generalizaci√≥n a nuevos datos

### Comportamiento esperado:
- **Train loss**: Puede ser un poco m√°s alto (pero centrado en voxeles importantes)
- **Val loss**: Deber√≠a ser **significativamente menor**
- **Visualmente**: Predicciones m√°s limpias en core, periferia m√°s ruidosa pero esperado

---

## üîç Diferencias clave vs Simple

| Aspecto | Simple | Weighted |
|---------|--------|----------|
| **PDD Filtering** | No | S√≠ (elimina 1% inferior) |
| **Loss Weighting** | Uniforme | Ponderado (core >> periferia) |
| **Robustez** | Sensible a ruido perif√©rico | Resistente a ruido |
| **Core Quality** | Buena | **Mejor** |
| **Periferia** | Intenta aprender ruido | Descarta gracefully |
| **Generalizaci√≥n** | Limitada | **Mejorada** |

---

## üìà Pr√≥ximos Pasos

1. **Entrenar modelo weighted** en cluster
2. **Comparar val_loss** con modelo simple
3. **Evaluar predicciones** con `eval_correct_norm.py`
4. **Analizar visualizaci√≥n** de slices (¬ømejor denoising en core?)
5. **Ajustar par√°metros** si es necesario:
   - Aumentar `LOW_WEIGHT` (0.5 ‚Üí 0.7) si periferia est√° completamente basura
   - Cambiar `DOSE_THRESHOLD` (0.20 ‚Üí 0.30) si queremos core m√°s grande
   - Cambiar `PERCENTILE_PDD_LOW` (1.0 ‚Üí 2.0) si queremos descartar m√°s capas

---

## ‚ö†Ô∏è Notas Importantes

- **El modelo weighted PENALIZA MENOS los errores en periferia**
  - Esto no es "trampa", es reconocer que periferia es principalmente ruido monte carlo
  - Diferente a IGNORAR, sigue aprendiendo pero con menos peso

- **Los pesos se calculan din√°micamente por patch**
  - Cada batch obtiene pesos basados en su m√°ximo local de dosis
  - Flexible y adaptativo

- **Compatible con normalizaci√≥n correcta**
  - Mantiene `max(input)` normalization
  - As√≠ modelo aprende amplificaci√≥n: small_input ‚Üí small_output, large_input ‚Üí large_output

---

## üìÇ Archivos Relacionados

- `train_weighted.py` - Script principal de entrenamiento
- `run_train_weighted.sh` - Script SLURM para cluster
- `compare_models.py` - Herramienta de comparaci√≥n
- `eval_correct_norm.py` - Evaluaci√≥n con normalizaci√≥n correcta
- `GT_LAYER_ANALYSIS.md` - An√°lisis capa-por-capa que motiv√≥ esto

---

**Status**: ‚úÖ Ready to train

**Next**: Execute `sbatch run_train_weighted.sh` en cluster

