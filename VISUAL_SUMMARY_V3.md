# üìä DeepMC v3: Resumen Visual de Cambios

## üî¥ Problema de v1 en Una Imagen

```
v1: MSE Standard (96% de datos son cero)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Distribuci√≥n de Voxeles en Volumen                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                      ‚îÇ
‚îÇ  96% CEROS (ruido fondo):  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚îÇ
‚îÇ  4% SE√ëAL (dosis real):    ‚ñà‚ñà                     ‚îÇ
‚îÇ                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Model MSE Loss:
  = mean((pred - target)¬≤)
  ‚âà mostly_zeros  ‚Üê DOMINA EL C√ÅLCULO
  
Conclusi√≥n trivial del optimizer:
  "Predecir ~0 en todas partes minimiza MSE global"
  
Resultado:
  ‚ùå Predicci√≥n plana (~60)
```

---

## üü¢ Soluci√≥n de v3: Loss Exponencial

```
v3: Exponential Loss (pesa voxeles con dosis)

weight(dose) = exp(dose/ref) - 1

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Pesos Aplicados por Dosis                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                      ‚îÇ
‚îÇ  dose = 0:      weight ‚âà 0.0      (bajo)         ‚îÇ
‚îÇ  dose = 500:    weight ‚âà 1.7      (moderado)     ‚îÇ
‚îÇ  dose = 1000:   weight ‚âà 7.4      (ALTO)         ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ  Core (1000): ‚úï7.4  ‚Üê MULTIPLICA EL ERROR        ‚îÇ
‚îÇ  Ruido (0):  ‚úï0.0   ‚Üê IGNORA PR√ÅCTICAMENTE       ‚îÇ
‚îÇ                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Conclusi√≥n del optimizer:
  "Errar en el core cuesta 7.4√ó m√°s"
  
Resultado:
  ‚úÖ Predicci√≥n estructura real
```

---

## üèóÔ∏è Cambios Arquitect√≥nicos

### v1: U-Net B√°sico
```
Input ‚Üí Conv ‚Üí Conv ‚Üí Conv ‚Üí ... ‚Üí Output
        
Problemas:
- Gradientes desaparecen en capas profundas
- No hay mecanismo de atenci√≥n (todos los canales igual)
- Sin normalizaci√≥n (entrenamiento inestable)
```

### v3: DeepMC-Style
```
Input [D+CT]  
  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Encoder (Residual Blocks)‚îÇ  ‚Üê Preservan gradientes
‚îÇ + SE Blocks             ‚îÇ  ‚Üê Atenci√≥n por canal
‚îÇ + Batch Norm            ‚îÇ  ‚Üê Estabilizaci√≥n
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚Üì
Bottleneck
  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Decoder (Residual Blocks)‚îÇ
‚îÇ + SE Blocks             ‚îÇ
‚îÇ + Batch Norm            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚Üì
Output = Input + Residual  ‚Üê Aprender correcciones
```

Ventajas:
- ‚úÖ Gradientes fluyen sin desvanecerse
- ‚úÖ Red aprende qu√© canales importan (SE blocks)
- ‚úÖ Entrenamiento estable (batch norm)
- ‚úÖ Aprender "delta" es m√°s f√°cil (residual)

---

## üìä Tabla Comparativa

| Aspecto | v1 | v3 |
|---------|----|----|
| **Loss** | MSE lineal | Exponencial |
| **Mask** | No | S√≠ (voxels > 0) |
| **Input Channels** | 1 | 2 (ready) |
| **Encoder Block** | Simple Conv | Residual + SE + BN |
| **Skip Connections** | Concat solo | Residual + Concat |
| **Output** | Absolute | Input + Residual |
| **Predicci√≥n Esperada** | Plana (~60) | Estructura (0-1000) |
| **PSNR Esperado** | <20 dB | >30 dB |

---

## üéØ Flujo de Entrenamiento

```
Dataset (80 muestras √ó 4 niveles)
  ‚Üì
Random patching (96¬≥ voxeles)
  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Epoch 1-10: Loss baja r√°pidamente        ‚îÇ
‚îÇ Modelo aprende estructura b√°sica         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Epoch 10-30: Mejora gradual              ‚îÇ
‚îÇ Fine-tuning de detalles                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Epoch 30-50: Plateau                     ‚îÇ
‚îÇ Early stopping se dispara (patience=20)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
  ‚Üì
Best Model Saved
  ‚Üì
Evaluation (PDD, PSNR, SSIM)
```

---

## üî¨ Hip√≥tesis Detr√°s de Cada Pilar

### Pilar 1: Loss Exponencial
**Hip√≥tesis**: El optimizer ignora el 96% de ruido si no se le da instrucciones

**Implementaci√≥n**: Pesar voxeles por dosis

**Validaci√≥n**: Si funciona, PDD debe mostrar estructura (no plana)

### Pilar 2: Entrada Dual
**Hip√≥tesis**: Sin contexto geom√©trico, impossible reconstruir f√≠sica

**Implementaci√≥n**: Concatenar [Dosis, CT]

**Validaci√≥n**: Con CT, error debe bajar 10-20%

**Status**: Deshabilitado (esperar dataset completo)

### Pilar 3: Arquitectura Avanzada
**Hip√≥tesis**: U-Net b√°sico no es suficiente para 96% ruido

**Implementaci√≥n**: Residual + SE + BatchNorm

**Validaci√≥n**: Training debe ser m√°s estable (sin NaN)

### Pilar 4: Data Strategy
**Hip√≥tesis**: 80 muestras es poco pero suficiente con augmentation

**Implementaci√≥n**: Random patching + 100 √©pocas

**Validaci√≥n**: Convergencia en 30-50 √©pocas (no overfitting r√°pido)

---

## üìà M√©tricas de √âxito

### ‚úÖ El Modelo Funciona Si:
1. **Val Loss**: Baja y se estabiliza (no diverge)
2. **PSNR**: > 30 dB (vs <20 en v1)
3. **SSIM**: > 0.85 (estructura preservada)
4. **PDD Shape**: Sigue GT (campaniforme, no plana)
5. **High Dose Error**: < Mid Dose < Low Dose
6. **Early Stopping**: Se activa ~epoch 30-50

### ‚ùå El Modelo Falla Si:
1. **Loss NaN**: Gradientes inestables
2. **PSNR**: < 20 dB (no mejora vs v1)
3. **PDD Plana**: Misma predicci√≥n constante
4. **No converge**: Loss sigue alto despu√©s de 50 √©pocas
5. **OOM**: Excede memoria GPU

---

## üöÄ Timeline Esperado

```
T+0h:    Iniciar entrenamiento
T+0.5h:  Epoch 1-5, val_loss baja
T+1h:    Epoch 15-20, empieza fine-tuning
T+1.5h:  Epoch 30-40, cerca del plateau
T+2h:    Epoch 45-50, early stopping
T+2.1h:  Guardar best_model.pt ‚úÖ
T+2.2h:  python evaluate_deepmc_v3.py
T+2.3h:  Resultados listos (PSNR, SSIM, PDD)
```

---

## üí° Key Insight

**El problema NO era que el modelo fuera incapaz.**

El modelo es capaz de aprender. El problema era que **le dimos los incentivos equivocados** (MSE est√°ndar).

v3 **cambia los incentivos** (loss exponencial) para que el modelo aprenda lo correcto.

Es un ejemplo perfecto de c√≥mo en ML, **la funci√≥n objetivo es cr√≠tica**:
- Objetivo incorrecto ‚Üí Soluciones malas (v1 predice cero)
- Objetivo correcto ‚Üí Soluciones buenas (v3 aprende estructura)

